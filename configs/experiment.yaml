name: titanic_baseline
seed: 42
debug_mode: false
debug_n_rows: null

# Model configuration
#model_name: random_forest
#model_params:
#  n_estimators: 800
#  max_depth: 8
#  min_samples_split: 5
#  min_samples_leaf: 2
#  random_state: 42
#  n_jobs: -1

# add gradient_boosting model with specific parameters
model_name: gradient_boosting
model_params:
  n_estimators: 84
  learning_rate: 0.1326731327153492
  max_depth: 3
  min_samples_split: 4
  min_samples_leaf: 2
  subsample: 0.8348468611017669
  random_state: 42

#Add xgboost model with specific parameters
#model_name: xgboost
#model_params:
#    n_estimators: 453
#    max_depth: 9
#    learning_rate: 0.14515585116869492
#    subsample: 0.7006075955571237
#    colsample_bytree: 0.8410144763171868
#    reg_alpha: 0.6797709737547473
#    reg_lambda: 0.9453885209468716
#    min_child_weight: 7
#    random_state: 42



# Cross-validation configuration
cv_folds: 5
cv_strategy: stratified
cv_shuffle: true
cv_random_state: 42

# Training configuration
early_stopping_rounds: null

# Logging
logging_level: INFO

# Optional: multi-model ensemble configuration for a single run
# When provided, the trainer will train each model in this list and
# save per-model OOF predictions and fold checkpoints. The predict
# command will ensemble these models per-fold at inference time.
ensemble:
  model_list:
    - name: random_forest
      params: { n_estimators: 400, max_depth: 8, random_state: 42 }
    - name: xgboost
      params: { n_estimators: 300, max_depth: 4, learning_rate: 0.08, subsample: 0.8, colsample_bytree: 0.8, random_state: 42 }
  method: average
  weights: null
